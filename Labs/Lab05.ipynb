{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear averaging dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G=(V,E,W)$ be a weighted graph, and $x(t) \\in \\mathrm{R}^{V}$ denote the state of the nodes of the graph.\n",
    "\n",
    "The dynamics of $x(t)$ reads\n",
    "\n",
    "$$\n",
    "x(t+1) = Px(t),\n",
    "$$\n",
    "\n",
    "where $P$ is the normalized adjacency matrix.\n",
    "\n",
    "**Theorem**: assume that\n",
    "- its condensation graph has 1 sink;\n",
    "- the sink component of the graph is aperiodic;\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\lim_{t \\to +\\infty} x(t) = \\alpha \\mathbf{1},\n",
    "$$\n",
    "\n",
    "i.e., the agents get to consensus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed of convergence\n",
    "\n",
    "Let us now work with undirected (the following argument holds only for undirected) connected graphs. If the graph is also aperiodic, the dynamics is guaranteed to converge to consensus.\n",
    "\n",
    "Let $\\lambda:=\\max \\{\\lambda_2,|\\lambda_n|\\}$, where $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ are the eigenvalues of $P$. $\\lambda_n \\ge -1$. \n",
    "\n",
    "The distance from consensus at time $t$ is in some sense proportional to $\\lambda^t$ (see lecture notes for more details).\n",
    "\n",
    "If $\\lambda$ is close to $1$, then the convergence is slow, whereas if $\\lambda$ is smaller the convergence is faster.\n",
    "\n",
    "Note that for strongly connected aperiodic graphs, by Perron-Frobenius theorem, we have $\\lambda_2 \\neq 1, \\lambda_n \\neq -1$ (thus $\\lambda \\neq 1$), hence the convergence to consensus is always achieved. Instead, if a graph is bipartite (which implies it is periodic), then $\\lambda_n = -1$, hence consensus is not achieved. Those facts are coherent with the theory of consensus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: note that every periodic strongly connected graph can be made periodic by adding at least a selfloop in the graph.\n",
    "\n",
    "To avoid periodicity, sometimes it is useful to introduce the **lazy dynamics** $x(t+1) = x(t)$, obtained by replacing $P$ with \n",
    "\n",
    "$$\n",
    "P_{lazy}=\\frac{P+\\mathbf{I}}{2}\n",
    "$$\n",
    "\n",
    "This is equivalent to adding selfloops to each node in the graph with weight equivalent to the degree of the node itself.\n",
    "\n",
    "An interpretation for the lazy dynamics is that nodes have some inertia in the opinion. Instead of averaging over the opinions of the neighbors, they also take into account their opinion at the previous step. In fact,\n",
    "\n",
    "$$\n",
    "x_i(t+1) = \\frac{x_i(t) + \\sum_{j} P_{ij} x_j(t)}{2}.\n",
    "$$\n",
    "\n",
    "Let us go back to the initial periodic example, and show that the lazy dynamics converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.generators.lattice.grid_graph(dim=[2,2])\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pl = P/2 + np.diag(np.ones(4))/2\n",
    "\n",
    "x = np.array([1, 0, 0, 1])\n",
    "\n",
    "for n in range(9):\n",
    "    x = Pl @ x\n",
    "print(\"x(10):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How fast is the convergence of lazy dynamics?\n",
    "\n",
    "Note that $\\lambda_n(P_{lazy}) = 1/2 + \\lambda_n(P)/2 \\ge 1/2 + (-1/2) = 0$.\n",
    "\n",
    "Thus, for lazy dynamics $\\lambda = \\lambda_2$. \n",
    "\n",
    "In the lazy dynamics, the speed convergence is governed by $\\lambda_2$. Let us now define the relaxation time as\n",
    "\n",
    "$$\n",
    "\\tau_{rel} = \\frac{1}{1-\\lambda_2}\n",
    "$$\n",
    "\n",
    "$\\lambda_2$ may be related to the level of connectedness of the graph. In particular, if the graph is well connected, $\\lambda_2$ is smaller and the convergence to consensus is faster.\n",
    "\n",
    "Let us consider a **cycle graph** with 1000 nodes.\n",
    "\n",
    "For the cycle graph one can show that\n",
    "\n",
    "$$\n",
    "\\lambda_2(P) = \\cos \\frac{2\\pi (n-1)}{n}.\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to + \\infty} \\lambda(P_{lazy}) = \\frac{1}{2} + \\lim_{n \\to + \\infty} \\frac{1}{2} \\cos \\frac{2\\pi}{n} = \\frac{1}{2} + \\frac{1}{2}\\left(1-\\frac{4 \\pi^2}{n^2}\\right) = 1 - 2\\frac{\\pi^2}{n^2},\n",
    "$$\n",
    "\n",
    "and the relaxation time for the lazy dynamics on the cycle is\n",
    "\n",
    "$$\n",
    "\\tau_{rel} = \\frac{n^2}{2\\pi^2}.\n",
    "$$\n",
    "\n",
    "This means that the convergence is achieved exponentially fast with a rate that scales with $n^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.cycle_graph(1000)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# Construct lazy P\n",
    "Pl = P/2 + np.diag(np.ones(1000))/2\n",
    "\n",
    "# let us start with random initial conditions\n",
    "x = np.random.rand(1000)\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.001):\n",
    "    x = Pl @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "\n",
    "print('Number of iteration for convergence:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the complete graph, which is the most connected graph by definition. For the complete graph,\n",
    "\n",
    "$$\n",
    "W=\\mathbf{1}\\mathbf{1}'-I.\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{1}\\mathbf{1}'$ has equal columns, it has rank $1$. Thus $\\mathbf{1}\\mathbf{1}'$ has $n-1$ eigenvalues equal to $0$. The remaining eigenvalue can be found by using the fact that the sum of the eigenvalues is the trace of the matrix. Since $=\\mathbf{1}\\mathbf{1}'-I$, the spectrum of $W$ is\n",
    "\n",
    "$$\n",
    "\\sigma_W = \\{n-1,-1,\\cdots,-1\\}\n",
    "$$\n",
    "\n",
    "Since the complete graph is regular and every node has degree $n-1$, $P=W/(n-1)$\n",
    "Thus, the spectrum of $P$ is\n",
    "\n",
    "$$\n",
    "\\sigma_P = \\{1,-\\frac{1}{n-1},\\cdots,-\\frac{1}{n-1}\\},\n",
    "$$\n",
    "\n",
    "and $P_{lazy}$ has spectrum\n",
    "\n",
    "$$\n",
    "\\sigma_{P_{lazy}} = \\{1,-\\frac{1}{2(n-1)}+\\frac{1}{2},\\cdots,-\\frac{1}{2(n-1)}+\\frac{1}{2}\\}\n",
    "$$\n",
    "\n",
    "This implies that $\\lambda_2 = \\frac{1}{2}-\\frac{1}{2(n-1)} = \\frac{n-2}{2(n-1)}$ and the relaxation time for large $n$ in the complete graph tends\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to +\\infty} \\tau_{rel} = 2,\n",
    "$$\n",
    "\n",
    "i.e., even for infinite $n$ the relaxation time is finite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.complete_graph(1000)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# Construct lazy P\n",
    "Pl = P/2 + np.diag(np.ones(1000))/2\n",
    "\n",
    "# let us start with random initial conditions\n",
    "x = np.random.rand(1000)\n",
    "\n",
    "variance = np.var(x)\n",
    "n=0\n",
    "\n",
    "while (variance>0.001):\n",
    "    x = Pl @ x\n",
    "    n=n+1\n",
    "    variance = np.var(x)\n",
    "\n",
    "print('Number of iteration for convergence:', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio bottleneck\n",
    "\n",
    "**Definition**: the ratio bottleneck (for simple graphs) is\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\t\\Phi = \\ & \\underset{\\substack{\\mathcal{U} \\subset \\mathcal{V}: \\\\ 0<w_{\\mathcal{U}} \\le \\frac{1}{2} \\mathbf{1}' w}}{\\min}\n",
    "\t& & \\frac{|\\partial_{\\mathcal{U}}|}{w_{\\mathcal{U}}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The ratio bottleneck is a measure of connectedness of the graph, and is related to $\\lambda_2$. The higher $\\lambda_2$ is, the less connected the graph is, the smaller is $\\Phi$, and the slower the convergence is. In particular,\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\Phi^2 \\le 1-\\lambda_2 \\le 2 \\Phi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: complete graph**\n",
    "\n",
    "The properties of a subset $\\mathcal{U}$ are completeley determined by its cardinality. Let us consider a complete graph with $n$ nodes, and let $\\mathcal{U}_m$ denote an arbitrary subset of $m$ nodes of the complete graph. Its boundary has cardinality $|\\partial_{\\mathcal{U}_m}| = m(n-m)$, thus\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\t\\Phi = \\ & \\underset{m \\le n/2}{\\min}\n",
    "\t& & \\frac{m(n-m)}{m(n-1)} \\rightarrow \\frac{1}{2}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "as $n \\to +\\infty$, and\n",
    "\n",
    "$$\n",
    "\\frac{1}{8} \\le 1-\\lambda_2 \\le 1, \\qquad 8 \\ge \\tau \\ge 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: barbell graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.barbell_graph(5,0)\n",
    "\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottleneck is clearly to select one of the two complete components of the graph.\n",
    "For that $\\mathcal{U}$,\n",
    "\n",
    "$$\n",
    "|\\partial_{\\mathcal{U}} = 1|, \\quad w_{\\mathcal{U}} = n(n-1)+1,\n",
    "$$\n",
    "\n",
    "where $2n$ is the order of the graph. Thus, for large $n$\n",
    "\n",
    "$$\n",
    "\\Phi = \\frac{1}{n^2-n+1} \\rightarrow 0,\n",
    "$$\n",
    "\n",
    "which implies that $\\lambda_2 \\to 1$, thus the relaxation time diverges for large $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: is the convergence always slow? \n",
    "    \n",
    "**Answer**: No, it depends on the initial condition.\n",
    "\n",
    "While $\\lambda_2$ describes a worst-case scenario for the speed of convergence, it might be that for some initial condition the convergence is fast!\n",
    "\n",
    "For instance, let us compare a random initial condition with an initial condition which has a high disagreement along the two components of the bottleneck of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.barbell_graph(200,0)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# Construct lazy P\n",
    "Pl = P/2 + np.diag(np.ones(G.number_of_nodes()))/2\n",
    "\n",
    "# let us start with an initial condition in which each complete subgraphs has (almost) half zeros and half ones\n",
    "# x = np.random.rand(G.number_of_nodes())>1/2\n",
    "x = np.zeros(G.number_of_nodes())\n",
    "for i in range(G.number_of_nodes()):\n",
    "    if i % 2 == 0:\n",
    "        x[i] = 1\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.001):\n",
    "    x = Pl @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "\n",
    "print('Number of iteration for convergence:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us start with a specific initial condition which maximizes the disagreement along the bottleneck of the graph, i.e.,\n",
    "# a complete subgraph has all zeros, and the second one has all ones.\n",
    "\n",
    "x = np.zeros(G.number_of_nodes())\n",
    "x[0:int(G.number_of_nodes()/2)] = np.ones(int(G.number_of_nodes()/2))\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.001):\n",
    "    x = Pl @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "\n",
    "print('Number of iteration for convergence:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though $\\lambda_2 \\to 1$, with a random initial condition the two complete graphs mix the opinions quickly and reach an agreement within the component in a few iterations. On average, both the components tend to opinion $1/2$, which mean that there is no large disagreements between the two components.\n",
    "\n",
    "On the other hand, if a component has opinion $0$ and the other one has opinion $1$, since the two complete components are not well connected, it takes a lot to reach consensus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to (not lazy) averaging dynamics\n",
    "\n",
    "In lazy dynamics the speed of convergence is described by $\\lambda_2$. In standard averaging dynamics, also $\\lambda_n$ plays a role. We recall indeed that the speed of convergence is described by\n",
    "\n",
    "$$\n",
    "\\lambda := \\max\\{\\lambda_2,|\\lambda_n|\\},\n",
    "$$\n",
    "\n",
    "where $\\lambda_n$ is the smallest eigenvalue of $P$.\n",
    "\n",
    "While $\\lambda_2$ is related to the level of connectedness of the graph, we shall see in a qualititive manner the role of $\\lambda_n$ by examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us construct a regular graph with 6m nodes, and degree of each node equal to m\n",
    "m = 5\n",
    "G = nx.cycle_graph(6*m)\n",
    "\n",
    "n_nodes = 6*m\n",
    "\n",
    "for n in range(6*m):\n",
    "    for i in range(m):\n",
    "        G.add_edge(n,(6*(i+1)+n-3) % 18)\n",
    "\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "nx.draw(G,pos,with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,v = np.linalg.eig(P)\n",
    "w = w.real\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is bipartite, hence $-1$ is in the spectrum and the convergence to consensus is not guaranteed.\n",
    "\n",
    "Let us now add a single edge to break the periodicity while mantaining the total number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(0,2)\n",
    "G.remove_edge(0,9)\n",
    "\n",
    "nx.draw(G,pos,with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "w,v = np.linalg.eig(P)\n",
    "w = w.real\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is not periodic now. However, $|\\lambda_n| \\sim 1$, which implies that the convergence of the dynamics is slow. On the other hand, $\\lambda_2$ is \"small\" ($\\sim 1/2$), thus the lazy dynamics converges quickly.\n",
    "\n",
    "In some sense, $|\\lambda_n| \\sim 1$ reflects the fact that the graph is almost periodic. Let us see this with an example, by comparing the speed in convergence of the lazy dynamics and the standard dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a larger graph\n",
    "m = 50\n",
    "G = nx.cycle_graph(6*m)\n",
    "\n",
    "G.add_edge(0,2)\n",
    "\n",
    "n_nodes = 6*m\n",
    "\n",
    "for n in range(6*m):\n",
    "    for i in range(m):\n",
    "        G.add_edge(n,(6*(i+1)+n-3) % n_nodes)\n",
    "        \n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# let us start with random initial condition and standard dynamics\n",
    "x0 = np.random.rand(G.number_of_nodes())>1/2\n",
    "x = x0\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.00001):\n",
    "    x = P @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "    if t>100000:\n",
    "        t='inf'\n",
    "        break\n",
    "        \n",
    "print('Number of iterations for convergence of standard dynamics:', t)\n",
    "\n",
    "# same initial condition, lazy dynamics\n",
    "x = x0\n",
    "\n",
    "# Construct lazy P\n",
    "Pl = P/2 + np.diag(np.ones(G.number_of_nodes()))/2\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.00001):\n",
    "    x = Pl @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "    if t>100000:\n",
    "        t='inf'\n",
    "        break\n",
    "        \n",
    "print('Number of iterations for convergence of lazy dynamics:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the speed of convergence in the lazy dynamics depends only on the connectivity of the graph ($\\lambda_2$), the speed of convergence in the standard dynamics depends both on the connectivity ($\\lambda_2$) and the periodicity of the graph ($|\\lambda_n|$). The example above shows that the standard averaging dynamics may converge slowly when the graph is a perturbation of a periodic graph, even if the graph is well-connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: distributed computation of average\n",
    "\n",
    "Let the node set describe a set of sensors that are deployed in some regions in order to collect measurements of some quantity of interest (for example, the temperature). \n",
    "\n",
    "Assume that these sensors have limited communication and computation capabilities that allow each of them to exchange information only with those other sensors that are close enough in space. \n",
    "\n",
    "Let the graph $G = (V, E)$ describe the pattern of vicinity among the sensors $i$ and $j$ so that there is an undirected link between node $i$ and node $j$ if they can communicate to each other (possibly using link weights decreasing with distance). Then, one can design a distributed algorithm for computing the average of the sensor's measurements based on the averaging dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i(0)$ be the measurement of each node. \n",
    "\n",
    "We are interested in designing an iterative distributed algorithm that allows the nodes to compute\n",
    "\n",
    "$$\n",
    "x = \\frac{1}{n}\\sum_i x_i(0)\n",
    "$$\n",
    "\n",
    "**First attempt**: we run a consensus algorithm. Since the graph is undirected, $\\pi_i = \\frac{w_i}{\\sum_j w_j}$. Thus, the algorithm converges to a consensus $\\alpha \\mathbf{1}$ such that\n",
    "\n",
    "$$\n",
    "\\alpha = \\sum_{i} \\frac{w_i}{w} x_i(0),\n",
    "$$\n",
    "\n",
    "which differs from our goal.\n",
    "\n",
    "If each node knows its degree $w_i$, each node can rescale its initial state, i.e., $y_i(0) = \\frac{x_i(0)}{w_i}$. The consensus algorithm for the variable $y_i$ thus converges to\n",
    "\n",
    "$$\n",
    "\\alpha_y = \\sum_{i} \\frac{w_i}{w} y_i(0) = \\frac{1}{w} \\sum_{i} x_i(0).\n",
    "$$\n",
    "\n",
    "If we assume that each node knows the average degree of the network $\\overline{w}$, thus\n",
    "\n",
    "$$\n",
    "x = \\alpha_y \\frac{w}{n} = \\alpha_y \\overline{w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Fix node positions on all pictures according to spring layout\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw_networkx(G, pos)\n",
    "\n",
    "n_nodes = len(G)\n",
    "\n",
    "x = np.random.rand(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us run the consensus algorithm for y\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "y = x/degrees\n",
    "\n",
    "for t in range(1000):\n",
    "    y = P @ y\n",
    "\n",
    "print(\"average initial condition:\", np.mean(x))\n",
    "# choose arbitrarly the first node, but all the nodes reach consensus on y\n",
    "print(\"average computed distributively\", y[0] * np.sum(degrees) / n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works! Unfortunately, requiring that each node knows the average degree of the network is not realistic and is not distributed, because it requires that each node has some global information on the network.\n",
    "\n",
    "However, there exists another way to solve the problem in a distributed manner.\n",
    "\n",
    "We run a second averaging dynamics, with initial condition $z_i(0) = \\frac{1}{w_i}$.\n",
    "\n",
    "This converges to\n",
    "\n",
    "$$\n",
    "\\lim_{t \\to + \\infty} z_i(t) = \\sum_i z_i(0) \\frac{w_i}{w} = \\sum_{i} \\frac{1}{w} = n/w = 1/\\overline{w}\n",
    "$$\n",
    "\n",
    "By combining the two, each node can estimate the average estimate by \n",
    "\n",
    "$$\n",
    "\\frac{\\lim_{t \\to + \\infty} y_i(t)}{\\lim_{t \\to + \\infty} z_i(t)} = \\alpha_y / \\overline{w} = x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us implement this\n",
    "\n",
    "z = 1/degrees\n",
    "\n",
    "for t in range(1000):\n",
    "    z = P @ z\n",
    "\n",
    "print(\"average state:\", np.mean(x))\n",
    "# choose arbitrarly the first node, but all the nodes reach consensus both on y and z\n",
    "print(\"average computed distributively\", y[0] / z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear flow dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of the lab we study how to simulate the linear averaging dynamics on graphs, which is the dual of linear averaging dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G=(V,E,W)$ be a weighted graph, and $x(t) \\in \\mathrm{R}^{V}$ denote the total mass in the node of the graph.\n",
    "\n",
    "The dynamics of $x(t)$ reads\n",
    "\n",
    "$$\n",
    "x(t+1) = P'x(t),\n",
    "$$\n",
    "\n",
    "where $P$ is the normalized adjacency matrix of the graph and is called in this context the **routing matrix**.\n",
    "The routing matrix has the following interpretation: if $x_j$ is the total mass in node $j$, $P_{ji}$ is the fraction of the mass that will be routed to node $i$ at the next step, i.e.,\n",
    "\n",
    "$$\n",
    "x_i(t+1) = \\sum_{j} P_{ji} x_j(t)\n",
    "$$\n",
    "\n",
    "**Theorem**: given a graph, assume that\n",
    "- its condensation graph has 1 sink;\n",
    "- the sink component of the graph is aperiodic;\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\lim_{t \\to +\\infty} x(t) = \\beta \\pi, \\quad \\beta = \\mathbf{1}' x(0)\n",
    "$$\n",
    "\n",
    "i.e., the mass distribution will eventually converge to the invariant distribution centrality of the network, where the proportionality factor can be computed by using the fact that the total mass on the network is preserved, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathbf{1}' x(t) = \\mathbf{1}' P' x(t-1) = \\mathbf{1}' x(t-1).\n",
    "$$\n",
    "\n",
    "**Observation**: note that this problem is the dual of the averaging dynamics, in the sense that in the averaging $\\pi' x(t)$ is constant and the dynamics converges to a state proportional to $\\mathbf{1}$, whereas in the flow dynamics $\\mathbf{1}' x(t)$ is constant and the dynamics converges to a state proportional to $\\pi$.\n",
    "\n",
    "**Observation**: note that the Theorem implies that under some conditions on the graph the invariant distribution centrality may be compute in a distributed way, by running the linear flow dynamics and normalizing the asymptotic state of the dynamics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear flow dynamics with multiple sink components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(1,2),(2,1),(1,0),(0,2),(3,2),(3,4),(4,5),(5,4),(6,4),(5,6)])\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw(G, pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,2,3,1,6])\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "x = P.T @ x\n",
    "print(\"x(1):\", x)\n",
    "\n",
    "x = P.T @ x\n",
    "print(\"x(2):\", x)\n",
    "\n",
    "x = P.T @ x\n",
    "print(\"x(3):\", x)\n",
    "\n",
    "x = P.T @ x\n",
    "print(\"x(4):\", x)\n",
    "\n",
    "x = P.T @ x\n",
    "print(\"x(5):\", x)\n",
    "\n",
    "x = P.T @ P.T @ P.T @ P.T @ P.T @ x\n",
    "print(\"x(10):\", x)\n",
    "\n",
    "x = P.T @ P.T @ P.T @ P.T @ P.T @ x\n",
    "print(\"x(15):\", x)\n",
    "\n",
    "x = P.T @ P.T @ P.T @ P.T @ P.T @ x\n",
    "print(\"x(20):\", x)\n",
    "\n",
    "x = P.T @ P.T @ P.T @ P.T @ P.T @ x\n",
    "print(\"x(25):\", x)\n",
    "\n",
    "x = P.T @ P.T @ P.T @ P.T @ P.T @ x\n",
    "print(\"x(30):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** Note that in node 3 the mass converges to 0. Why?\n",
    "\n",
    "**Question 2** Can you use the asymptotic state of the dynamics to deduce all the dominant eigenvectors of the graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute left dominant eigenvectors (invariant distributions) by linear flow dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least a non-zero element in x[0], x[1], x[2] (first trapping set)\n",
    "# at least a non-zero element in x[4], x[5], x[6] (second trapping set)\n",
    "x = np.array([1,0,3,2,3,1,6])\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray()\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "for iter in range(100):\n",
    "    x = P.T @ x\n",
    "\n",
    "print(\"x(100):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above has two sink components (or trapping sets), both aperiodic.\n",
    "\n",
    "Let us recall some properties on left dominant eigenvectors of $P$.\n",
    "\n",
    "**Left dominant eigenvectors**: recall that \n",
    "- the eigenvalue 1 has multeplicity (both geometric and algebraic) equal to the number of sinks of the condensation graph;\n",
    "- all the left dominant eigenevctors of $P$ has support on the nodes that belong to the trapping sets of the graph;\n",
    "- dominant eigenvectors whose support belong to one trapping set only are called extremal;\n",
    "- all the dominant eigenvectors can be obtained as a convex combination of extremal dominant eigenvectors.\n",
    "\n",
    "To compute all the dominant eigenvectors, we run the dynamics with a non-zero mass in both the trapping sets of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mass distribution in the trapping sets reaches an equilibrium (because both the trapping sets are aperiodic). \n",
    "Note indeed that eventually all the mass will flow out from non-trapping sets ($x[3] \\to 0$). Once $x$ approaches zero for all the nodes not belonging to trapping sets, the dynamics in each trapping set becomes independent of the rest of the network. Thus, we can use theoretical results to guarantee that if a trapping set of the graph is aperiodic, then the dynamics on the subgraph will converge to the (unique) invariant distribution of the induced subgraph defined on the trapping set.\n",
    "\n",
    "If the dynamics reaches an equilibrium, the resulting $x^*$ has to be left dominant eigenvector. However, if we run another dynamics with different initial condition we get another left dominant eigenvector..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "print(\"Left dominant eigenvector:\", x/np.sum(x))\n",
    "\n",
    "x = np.array([0,0,1,0,3,1,6])\n",
    "\n",
    "for iter in range(100):\n",
    "    x = P.T @ x\n",
    "\n",
    "print(\"Left dominant eigenvector:\", x/np.sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot try an infinite number of initial conditions...\n",
    "\n",
    "To obtain all the dominant eigenvectors, we first obtain the two extremal ones $\\pi^{(1)}$ and $\\pi^{(2)}$, and then combine then by convex combination, i.e.,\n",
    "\n",
    "$$\n",
    "\\pi = \\alpha \\pi^{(1)} + (1-\\alpha) \\pi^{(2)}, \\quad \\alpha \\in [0,1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the two extremal eigenvectors, we run a dynamics with non-zero initial condition on a trapping set only, e.g., for the first trapping set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([7,2,1,0,0,0,0])\n",
    "\n",
    "for iter in range(100):\n",
    "    x = P.T @ x\n",
    "\n",
    "print(\"First extremal dominant eigenvector:\", x/np.sum(x))\n",
    "\n",
    "x = np.array([0,0,0,0,3,4,3])\n",
    "\n",
    "for iter in range(100):\n",
    "    x = P.T @ x\n",
    "\n",
    "print(\"Second extremal dominant eigenvector:\", x/np.sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear flow dynamics on graph whose condensation graph has a unique sink (but not strongly connected)\n",
    "\n",
    "Let us now add a link in such a way that the condensation graph has now a single sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(6,3)\n",
    "nx.draw(G, pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What kind of asymptotic state do you expect for this graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# define initial condition\n",
    "x = [1, 1, 0, 1, 1, 0, 0]\n",
    "\n",
    "for n in range(999):\n",
    "    x = P.T @ x\n",
    "print(\"x(1000):\", x, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "If the graph is not strongly connected, but its condensation graph has one sink:\n",
    "- the averaging dynamics converges to a consensus, whose value depends only on the initial state of nodes of the sink;\n",
    "- the linear flow dynamics converges to the dominant eigenvector of $P'$, which has support only on the nodes of the sink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duality of the problems\n",
    "We are given an aperiodic graph whose condensation graph has 1 sink. Let $x(0)$ be the initial state.\n",
    "\n",
    "We can compute the asymptotic consensus value without making computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from([0,1,2,3,4,5,6])\n",
    "G.add_edges_from([(1,2),(2,1),(1,0),(0,2),(3,2),(3,4),(4,5),(5,4),(6,4),(5,6),(6,3),(0,3)])\n",
    "# labels of nodes are couples: (column,row)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(7)\n",
    "\n",
    "# compute \\pi by running the mass dynamics with a normalized initial condition y\n",
    "# the asymptotic state will be exactly \\pi.\n",
    "\n",
    "y = np.random.rand(7)\n",
    "y = y/np.sum(y)\n",
    "\n",
    "for iter in range(500):\n",
    "    y = P.T @ y\n",
    "\n",
    "# the consensus value is pi.T @ x, but y(t) --> \\pi, thus we can use y instead of \\pi.\n",
    "print(\"The consensus value:\", y.T @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result by running the averaging dynamics\n",
    "for iter in range(500):\n",
    "    x = P @ x\n",
    "    \n",
    "print(\"The asymptotic state x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear averaging dynamics with stubborn agents\n",
    "We study how to simulate the linear averaging dynamics on graphs in presence of stubborn agents.\n",
    "\n",
    "We focus on the optimal placement problem, which consists of optimally chosing the position of a stubborn node on the graph in order to maximize its influence on the asymptotic outcome of the dynamics.\n",
    "\n",
    "Let us first summarize the theory in presence of stubborn agents.\n",
    "\n",
    "We are given a network, where the agents $V$ are divided in regular agents $R$ and stubborn agents $S$. The regular agents update their opinion $x(t)$ according to the standard DeGroot model, while the stubborn agents do not update their opinion $u(t) \\equiv u$.\n",
    "\n",
    "Let $Q=P|_{R \\times R}$ and $E=P|_{R \\times S}$.\n",
    "\n",
    "Thus, the dynamics for the regular agents read:\n",
    "\n",
    "$$\n",
    "x(t+1) = Qx(t) + Eu.\n",
    "$$\n",
    "\n",
    "Under some assumptions (see the lecture notes for details) the dynamics converges to \n",
    "\n",
    "$$\n",
    "x^* = (\\mathbf{I}-Q)^{-1}Eu.\n",
    "$$\n",
    "\n",
    "Note that:\n",
    "- the asymptotic state is not a consensus;\n",
    "- the asymptotic state does not depend on the initial opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We start by implementing the averaging dynamics with stubborn nodes. \n",
    "\n",
    "To illustrate this procedure, we will analyse the following example that involves a $3 \\times 4$ grid graph $\\mathcal G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.generators.lattice.grid_graph(dim=[3,4])\n",
    "n_nodes = len(G)\n",
    "print(\"Number of nodes:\", n_nodes)\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "nx.draw_spectral(G, with_labels=True)\n",
    "\n",
    "print(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary that maps the label of nodes  \n",
    "# (from (0,0) to (2,1)) to their index (from 0 to n_nodes-1)\n",
    "indices = dict()\n",
    "for i in range(n_nodes):\n",
    "    indices[list(G.nodes)[i]] = i\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "    \n",
    "# Stubborn and regular nodes\n",
    "stubborn = [(0,0), (3,2)];\n",
    "stubborn_id = [indices.get(key) for key in stubborn]\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")\n",
    "\n",
    "# Input to stubborn nodes\n",
    "u = [0,1]\n",
    "\n",
    "# P matrix\n",
    "A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "# Submatrices\n",
    "# Using ix_ one can construct index arrays that will \n",
    "# index a cross product. \n",
    "# a[np.ix_([1,3],[2,5])] returns the array [[a[1,2] a[1,5]], [a[3,2] a[3,5]]].\n",
    "Q = P[np.ix_(regular_id, regular_id)]\n",
    "E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "# Sample a random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial state:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "print(\"Final state:\")\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.average(x_final)\n",
    "print(\"Average asymptotic opinion:\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the dynamics does not converge to consensus. Moreover, in contrast with averaging without input stubborn nodes, we can verify that the asymptotic equilibrium does not depend on the initial condition. Furthermore, the dynamics converge to an equilibrium even though the graph is periodic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample another random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial state:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "print(\"Final state:\")\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])\n",
    "    \n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal placement of stubborn nodes\n",
    "Suppose that node $(0,0)$ is stubborn with opinion $u_{(0,0)}=0$. We want to find the optimal position $(i,j)$ of a stubborn node with opinion $1$ in order to maximize the asymptotic average opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple approach is to consider all possible positions $(i,j)$ and pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "\n",
    "# We will store final opinion vectors and \n",
    "# average of final opinions in dictionaries\n",
    "# where the key is the position (i,j) of the \n",
    "# 1-stubborn agent\n",
    "final_opinions = dict()\n",
    "average_opinion = dict() \n",
    "\n",
    "\n",
    "for (i,j) in G.nodes:\n",
    "    # Position (0,0) is occupied by the 0-stubborn node\n",
    "    if (i,j)==(0,0):\n",
    "        continue\n",
    "        \n",
    "    # Stubborn and regular nodes\n",
    "    stubborn = [(0,0), (i,j)];\n",
    "    stubborn_id = [indices.get(key) for key in stubborn]\n",
    "    regular = [node for node in G.nodes if node not in stubborn]\n",
    "    regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "    print(\"Stubborn nodes:\", stubborn)\n",
    "\n",
    "    # Input to stubborn nodes\n",
    "    u = [0,1]\n",
    "\n",
    "\n",
    "    # P matrix\n",
    "    A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "    A = A.toarray() # convert A to a numpy array\n",
    "    degrees = np.sum(A,axis=1)\n",
    "    D = np.diag(degrees)\n",
    "    P = np.linalg.inv(D) @ A\n",
    "\n",
    "    # Submatrices\n",
    "    Q = P[np.ix_(regular_id, regular_id)]\n",
    "    E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "    # Sample a random initial condition for regular nodes\n",
    "    ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "    # Set the initial condition for the dynamics\n",
    "    x = np.zeros((n_nodes,n_iter))\n",
    "    x[stubborn_id,0] = u;\n",
    "    x[regular_id,0] = ic;\n",
    "\n",
    "    for t in range(1,n_iter):\n",
    "        x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "        x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "    final_opinions[(i,j)] = x[:,n_iter-1]\n",
    "    average_opinion[(i,j)] = np.average(final_opinions[(i,j)])\n",
    "    print(\"Average opinion:\", average_opinion[(i,j)], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the dependence of the average asymptotic opinion on the position of the $1$-stubborn node we can plot the grid graph by setting each node's size and color according to the magnitude of the average asymptotic opinion when the $1$-stubborn is placed in such node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dummy (0,0) entry to the dictionary\n",
    "# to make its size = n_nodes\n",
    "average_opinion[(0,0)] = 0\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(10*average_opinion[node]) for node in G.nodes],\n",
    "        node_color= [average_opinion[node] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal placements of the 1-stubborn player are the maximizers of the final average opinion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the average opinion values from dict_values to numpy array\n",
    "avg = np.fromiter(average_opinion.values(),dtype=float)\n",
    "\n",
    "optimal_place = [place for place in average_opinion.keys() if average_opinion[place]==np.max(avg)]\n",
    "print(\"Optimal placements:\", optimal_place)\n",
    "\n",
    "print(optimal_place)\n",
    "\n",
    "# print the final opinions under optimal placement\n",
    "opt_final = final_opinions.get(*optimal_place)\n",
    "print(opt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the asymptotic opinions of the nodes when the stubborn is placed in (1,1)\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(8*opt_final[indices.get(node)]) for node in G.nodes],\n",
    "        node_color= [opt_final[indices.get(node)] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to an old example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.cycle_graph(10)\n",
    "G = nx.Graph.to_directed(G)\n",
    "G.remove_edges_from([(0,1),(0,9)])\n",
    "G.add_edge(0,0)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "n_nodes = G.number_of_nodes()\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "n_iter = 100\n",
    "x = np.zeros((10,n_iter))\n",
    "\n",
    "# set initial condition (1,0,0,0,0,0,0,0,0,0)\n",
    "x[0,0] = 1\n",
    "\n",
    "# evolve the states\n",
    "for t in range(1,n_iter):\n",
    "    x[:,t] = P @ x[:,t-1]\n",
    "\n",
    "print(\"Average final opinions:\", np.mean(x[:,n_iter-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(G.number_of_nodes()):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prove that this is equivalent to having node 0 stubborn with opinion 1.\n",
    "\n",
    "Indeed, note that because of the topology of the graph, the opinion of node 0 is not influenced by anyone. The same dynamics can be obtained by assuming that node 0 is stubborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stubborn and regular nodes\n",
    "stubborn = [0];\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")\n",
    "\n",
    "# Input to stubborn nodes\n",
    "u = [1]\n",
    "\n",
    "# Submatrices\n",
    "Q = P[np.ix_(regular, regular)]\n",
    "E = P[np.ix_(regular, stubborn)]\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn,0] = u;\n",
    "print(\"Initial condition:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular, t] = Q @ x[regular, t-1] + E @ x[stubborn, t-1]\n",
    "    x[stubborn, t] = x[stubborn, t-1];\n",
    "\n",
    "x_final = x[:,n_iter-1]\n",
    "print(x_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(G.number_of_nodes()):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same trajectory as before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more general model: overview on Friedkin-Johnsen model\n",
    "The French-DeGroot dynamics can be generalized by taking into account that each agents is not completely regular or completely stubborn. The opinion of each agents is in part due to \"innate\" opinions, and in part due to influence of society.\n",
    "\n",
    "The following opinion dynamics model is known as Friedkin-Johnsen model.\n",
    "Here:\n",
    "- $x_i(t)$ is the opinion of the agent $i$;\n",
    "- $y_i$ is the innate opinion of agent $i$.\n",
    "- $\\alpha_i$ is its level of stubborness, i.e., the level of confidence in his/her opinion $y_i$.\n",
    "\n",
    "The dynamics reads:\n",
    "\n",
    "$$\n",
    "x_i(t+1) = \\alpha_i y_i + (1-\\alpha_i) \\sum_{j} P_{ij} x_j(t).\n",
    "$$\n",
    "\n",
    "If $\\alpha = \\mathbf{0}$, we get the French-DeGroot model without input.\n",
    "\n",
    "If $\\alpha \\in \\{0,1\\}^{V}$, we get the French-DeGroot model with stubborn nodes.\n",
    "\n",
    "As the French-DeGroot model with input, also the Friedkin-Johnsen dynamics converges to a non-consensus state, which depends on $\\alpha, y$, but not on the initial opinions $x(0)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
